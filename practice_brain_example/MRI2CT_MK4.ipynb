{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6a5f81-b905-4381-bdc2-61bed823e021",
   "metadata": {},
   "source": [
    "# Model MK4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "109c03cc-aaa3-4f40-b0ea-44b262eeb1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 570 pairs, Size: (570, 256, 256) (input), (570, 256, 256) (output)\n",
      "Validation set: 90 pairs, Size: (90, 256, 256) (input), (90, 256, 256) (output)\n",
      "Testing set: 150 pairs, Size: (150, 256, 256) (input), (150, 256, 256) (output)\n",
      "Training set:\n",
      " - First MRI image size: (256, 256)\n",
      " - First CT image size: (256, 256)\n",
      " - All MRI images same size: True\n",
      " - All CT images same size: True\n",
      "\n",
      "Validation set:\n",
      " - First MRI image size: (256, 256)\n",
      " - First CT image size: (256, 256)\n",
      " - All MRI images same size: True\n",
      " - All CT images same size: True\n",
      "\n",
      "Testing set:\n",
      " - First MRI image size: (256, 256)\n",
      " - First CT image size: (256, 256)\n",
      " - All MRI images same size: True\n",
      " - All CT images same size: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance  # To calculate FID\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "# Load the .npy files\n",
    "train_input = np.load('data(processed)/train_input.npy').astype(np.float32)  # Convert to float32\n",
    "train_output = np.load('data(processed)/train_output.npy').astype(np.float32)  # Convert to float32\n",
    "val_input = np.load('data(processed)/val_input.npy').astype(np.float32)\n",
    "val_output = np.load('data(processed)/val_output.npy').astype(np.float32)\n",
    "test_input = np.load('data(processed)/test_input.npy').astype(np.float32)\n",
    "test_output = np.load('data(processed)/test_output.npy').astype(np.float32)\n",
    "\n",
    "# Custom Dataset Class\n",
    "class MRICTDataset(Dataset):\n",
    "    def __init__(self, input_data, output_data, transform=None):\n",
    "        self.input_data = input_data\n",
    "        self.output_data = output_data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load MRI and CT images\n",
    "        mri_image = self.input_data[idx]\n",
    "        ct_image = self.output_data[idx]\n",
    "\n",
    "        # Convert to 3-channel images\n",
    "        mri_image = np.repeat(mri_image[np.newaxis, :, :], 3, axis=0)  # Repeat for 3 channels\n",
    "        ct_image = np.repeat(ct_image[np.newaxis, :, :], 3, axis=0)  # Repeat for 3 channels\n",
    "\n",
    "        if self.transform:\n",
    "            mri_image = torch.tensor(mri_image)  # Convert to tensor\n",
    "            ct_image = torch.tensor(ct_image)  # Convert to tensor\n",
    "\n",
    "            mri_image = self.transform(mri_image)  # Transform MRI\n",
    "            ct_image = self.transform(ct_image)  # Transform CT\n",
    "\n",
    "        return mri_image, ct_image\n",
    "\n",
    "# Data transformation with augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.float()),  # Ensure tensor is float type\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize for 3 channels\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip for data augmentation\n",
    "    transforms.RandomVerticalFlip(),  # Random vertical flip for data augmentation\n",
    "])\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = MRICTDataset(train_input, train_output, transform=transform)\n",
    "val_dataset = MRICTDataset(val_input, val_output, transform=transform)\n",
    "test_dataset = MRICTDataset(test_input, test_output, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Changed batch size to 16\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)  # Changed batch size to 16\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # Changed batch size to 16\n",
    "\n",
    "# Print counts and sizes\n",
    "print(f'Training set: {len(train_dataset)} pairs, Size: {train_input.shape} (input), {train_output.shape} (output)')\n",
    "print(f'Validation set: {len(val_dataset)} pairs, Size: {val_input.shape} (input), {val_output.shape} (output)')\n",
    "print(f'Testing set: {len(test_dataset)} pairs, Size: {test_input.shape} (input), {test_output.shape} (output)')\n",
    "\n",
    "# Check image sizes and uniformity\n",
    "def check_image_sizes(dataset, name):\n",
    "    # Get the size of the first image\n",
    "    first_input_size = dataset.input_data[0].shape\n",
    "    first_output_size = dataset.output_data[0].shape\n",
    "    \n",
    "    # Check if all images have the same size\n",
    "    all_inputs_same_size = all(img.shape == first_input_size for img in dataset.input_data)\n",
    "    all_outputs_same_size = all(img.shape == first_output_size for img in dataset.output_data)\n",
    "\n",
    "    print(f\"{name} set:\")\n",
    "    print(f\" - First MRI image size: {first_input_size}\")\n",
    "    print(f\" - First CT image size: {first_output_size}\")\n",
    "    print(f\" - All MRI images same size: {all_inputs_same_size}\")\n",
    "    print(f\" - All CT images same size: {all_outputs_same_size}\\n\")\n",
    "\n",
    "# Check sizes for training, validation, and testing datasets\n",
    "check_image_sizes(train_dataset, \"Training\")\n",
    "check_image_sizes(val_dataset, \"Validation\")\n",
    "check_image_sizes(test_dataset, \"Testing\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Define device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f5a5d7-881b-4e09-9bd2-45f5666e5c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................[Epoch 1/60] [Batch 36/36] [D Loss: 14.273858] [G Loss: 7.524763] [GAN Loss: 0.911714] [Perceptual Loss: 0.146957] [D Real Acc: 0.800000] [D Fake Acc: 1.000000] [D Avg Acc: 0.900000]\n",
      "Validation -> \n",
      "[Epoch 1/60] Validation Metrics: MSE: 0.710287, SSIM: -0.000126, FID: 237.099681, VGG Loss: 0.208323\n",
      "....................................[Epoch 2/60] [Batch 36/36] [D Loss: 13.968480] [G Loss: 5.541185] [GAN Loss: 0.825027] [Perceptual Loss: 0.104804] [D Real Acc: 0.700000] [D Fake Acc: 1.000000] [D Avg Acc: 0.850000]\n",
      "Validation -> \n",
      "[Epoch 2/60] Validation Metrics: MSE: 0.723093, SSIM: 0.000432, FID: 276.891614, VGG Loss: 0.176223\n",
      "....................................[Epoch 3/60] [Batch 36/36] [D Loss: 14.154608] [G Loss: 5.001728] [GAN Loss: 0.531500] [Perceptual Loss: 0.099338] [D Real Acc: 0.700000] [D Fake Acc: 0.800000] [D Avg Acc: 0.750000]\n",
      "Validation -> \n",
      "[Epoch 3/60] Validation Metrics: MSE: 0.652915, SSIM: 0.001418, FID: 263.658696, VGG Loss: 0.169282\n",
      "....................................[Epoch 4/60] [Batch 36/36] [D Loss: 13.597912] [G Loss: 8.101479] [GAN Loss: 0.873997] [Perceptual Loss: 0.160611] [D Real Acc: 0.000000] [D Fake Acc: 1.000000] [D Avg Acc: 0.500000]\n",
      "Validation -> \n",
      "[Epoch 4/60] Validation Metrics: MSE: 0.753915, SSIM: 0.000148, FID: 231.997993, VGG Loss: 0.180210\n",
      "....................................[Epoch 5/60] [Batch 36/36] [D Loss: 12.398672] [G Loss: 4.787094] [GAN Loss: 0.539861] [Perceptual Loss: 0.094383] [D Real Acc: 0.000000] [D Fake Acc: 0.300000] [D Avg Acc: 0.150000]\n",
      "Validation -> \n",
      "[Epoch 5/60] Validation Metrics: MSE: 0.753951, SSIM: -0.000647, FID: 292.844599, VGG Loss: 0.165181\n",
      "....................................[Epoch 6/60] [Batch 36/36] [D Loss: 13.722673] [G Loss: 4.898558] [GAN Loss: 0.638977] [Perceptual Loss: 0.094657] [D Real Acc: 0.300000] [D Fake Acc: 0.400000] [D Avg Acc: 0.350000]\n",
      "Validation -> \n",
      "[Epoch 6/60] Validation Metrics: MSE: 0.746663, SSIM: -0.001362, FID: 313.976517, VGG Loss: 0.167425\n",
      "....................................[Epoch 7/60] [Batch 36/36] [D Loss: 11.929754] [G Loss: 4.910249] [GAN Loss: 0.748775] [Perceptual Loss: 0.092477] [D Real Acc: 0.000000] [D Fake Acc: 0.800000] [D Avg Acc: 0.400000]\n",
      "Validation -> \n",
      "[Epoch 7/60] Validation Metrics: MSE: 0.615572, SSIM: 0.000202, FID: 293.002207, VGG Loss: 0.157850\n",
      "....................................[Epoch 8/60] [Batch 36/36] [D Loss: 13.227072] [G Loss: 5.145198] [GAN Loss: 0.586261] [Perceptual Loss: 0.101310] [D Real Acc: 0.000000] [D Fake Acc: 0.400000] [D Avg Acc: 0.200000]\n",
      "Validation -> \n",
      "[Epoch 8/60] Validation Metrics: MSE: 0.539285, SSIM: 0.000752, FID: 279.982463, VGG Loss: 0.161053\n",
      "....................................[Epoch 9/60] [Batch 36/36] [D Loss: 11.431927] [G Loss: 4.391845] [GAN Loss: 0.388153] [Perceptual Loss: 0.088971] [D Real Acc: 0.700000] [D Fake Acc: 0.000000] [D Avg Acc: 0.350000]\n",
      "Validation -> \n",
      "[Epoch 9/60] Validation Metrics: MSE: 0.471294, SSIM: 0.001864, FID: 233.435664, VGG Loss: 0.157572\n",
      "....................................[Epoch 10/60] [Batch 36/36] [D Loss: 10.599083] [G Loss: 4.854816] [GAN Loss: 0.613980] [Perceptual Loss: 0.094241] [D Real Acc: 0.600000] [D Fake Acc: 0.300000] [D Avg Acc: 0.450000]\n",
      "Validation -> \n",
      "[Epoch 10/60] Validation Metrics: MSE: 0.452454, SSIM: 0.002540, FID: 233.037651, VGG Loss: 0.159414\n",
      "....................................[Epoch 11/60] [Batch 36/36] [D Loss: 12.452425] [G Loss: 4.291619] [GAN Loss: 0.967110] [Perceptual Loss: 0.073878] [D Real Acc: 0.000000] [D Fake Acc: 0.700000] [D Avg Acc: 0.350000]\n",
      "Validation -> \n",
      "[Epoch 11/60] Validation Metrics: MSE: 0.491460, SSIM: 0.002309, FID: 287.343836, VGG Loss: 0.153903\n",
      "....................................[Epoch 12/60] [Batch 36/36] [D Loss: 7.818775] [G Loss: 3.506533] [GAN Loss: 0.492228] [Perceptual Loss: 0.066985] [D Real Acc: 0.000000] [D Fake Acc: 0.200000] [D Avg Acc: 0.100000]\n",
      "Validation -> \n",
      "[Epoch 12/60] Validation Metrics: MSE: 0.446077, SSIM: 0.002788, FID: 252.953171, VGG Loss: 0.156313\n",
      "....................................[Epoch 13/60] [Batch 36/36] [D Loss: 8.581855] [G Loss: 3.933186] [GAN Loss: 0.686482] [Perceptual Loss: 0.072149] [D Real Acc: 0.600000] [D Fake Acc: 0.200000] [D Avg Acc: 0.400000]\n",
      "Validation -> \n",
      "[Epoch 13/60] Validation Metrics: MSE: 0.417255, SSIM: 0.004625, FID: 267.992976, VGG Loss: 0.154176\n",
      "."
     ]
    }
   ],
   "source": [
    "# Define UNet Generator\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = self.encoder_block(3, 64)\n",
    "        self.enc2 = self.encoder_block(64, 128)\n",
    "        self.enc3 = self.encoder_block(128, 256)\n",
    "        self.enc4 = self.encoder_block(256, 512)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.encoder_block(512, 512)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec4 = self.decoder_block(512, 256)\n",
    "        self.dec3 = self.decoder_block(512 + 256, 128)\n",
    "        self.dec2 = self.decoder_block(256 + 128, 64)\n",
    "        self.dec1 = nn.ConvTranspose2d(192, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.final_upsample = nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1)\n",
    "        self.tanh = nn.Tanh()  # Using Tanh for final output\n",
    "\n",
    "    def encoder_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.15),\n",
    "            nn.Dropout(p=0.1)  # Reduced dropout to 0.2 in the generator\n",
    "        )\n",
    "\n",
    "    def decoder_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.15),\n",
    "            nn.Dropout(p=0.1)  # Reduced dropout to 0.2 in the generator\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        bottleneck = self.bottleneck(enc4)\n",
    "\n",
    "        dec4 = self.dec4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec3 = self.dec3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec2 = self.dec2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec1 = self.dec1(dec2)\n",
    "        out = self.final_upsample(dec1)\n",
    "        return self.tanh(out)  # Use tanh for output\n",
    "\n",
    "# Define Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0)  # Output shape: [batch_size, 1, 1, 1]\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling to get single scalar value\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.sigmoid = nn.Sigmoid()  # Add Sigmoid layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        out = self.global_avg_pool(out)  # Apply global average pooling\n",
    "        out = out.view(out.size(0), -1)  # Flatten to [batch_size, 1]\n",
    "        out = self.sigmoid(out)  # Apply Sigmoid\n",
    "        return out\n",
    "\n",
    "# Loss functions\n",
    "criterion_GAN = nn.BCELoss()  # GAN loss\n",
    "criterion_pixelwise = nn.L1Loss()  # L1 loss\n",
    "vgg = models.vgg19(weights='DEFAULT').features.eval().to(device)  # Pre-trained VGG for perceptual loss\n",
    "\n",
    "# Frechet Inception Distance (FID) for validation\n",
    "fid_metric = FrechetInceptionDistance().to(device)\n",
    "\n",
    "# Custom perceptual loss function\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    # Compute the VGG features\n",
    "    y_true_features = vgg(y_true)\n",
    "    y_pred_features = vgg(y_pred)\n",
    "    return nn.functional.mse_loss(y_pred_features, y_true_features)  # Use MSE for feature loss\n",
    "\n",
    "# Function to compute gradient penalty\n",
    "def compute_gradient_penalty(discriminator, real_images, fake_images):\n",
    "    batch_size = real_images.size(0)\n",
    "    # Generate random weight for interpolation\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1).to(real_images.device)\n",
    "    # Interpolate between real and fake images\n",
    "    interpolates = (alpha * real_images + (1 - alpha) * fake_images).requires_grad_(True)\n",
    "    \n",
    "    # Get discriminator output for interpolated images\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    \n",
    "    # Compute gradients of the output with respect to the interpolated images\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(d_interpolates),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "\n",
    "    # Compute the L2 norm of the gradients\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# Hyperparameter for balancing losses\n",
    "lambda_l1 = 45  # Decreased lambda_l1\n",
    "lambda_gp = 15  # Weight for gradient penalty\n",
    "\n",
    "# Early stopping variables\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Initialize models\n",
    "generator = UNetGenerator().to(device)\n",
    "discriminator = Discriminator(in_channels=3).to(device)\n",
    "\n",
    "# Optimizers                                     step size = lr * gradient\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.000055, betas=(0.5, 0.999))  # Updated D learning rate\n",
    "\n",
    "# Learning rate scheduler with patience\n",
    "scheduler_G = optim.lr_scheduler.ReduceLROnPlateau(optimizer_G, mode='min', factor=0.5, patience=5)\n",
    "scheduler_D = optim.lr_scheduler.ReduceLROnPlateau(optimizer_D, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Validation function to calculate metrics\n",
    "def validate(generator, val_loader):\n",
    "    generator.eval()  # Set to evaluation mode\n",
    "    total_mse, total_ssim, total_fid, total_vgg_loss = 0, 0, 0, 0\n",
    "    fid_metric.reset()  # Reset FID metric\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mri_images, ct_images in val_loader:\n",
    "            mri_images, ct_images = mri_images.to(device), ct_images.to(device)\n",
    "\n",
    "            # Generate CT images from MRI\n",
    "            fake_ct_images = generator(mri_images)\n",
    "\n",
    "            # MSE\n",
    "            mse = mse_loss(fake_ct_images, ct_images).item()\n",
    "            total_mse += mse\n",
    "\n",
    "            # SSIM\n",
    "            ssim_val = 0\n",
    "            for i in range(fake_ct_images.shape[0]):\n",
    "                # Adjusting SSIM calculation\n",
    "                ssim_val += ssim(fake_ct_images[i].cpu().numpy(), ct_images[i].cpu().numpy(), \n",
    "                                  multichannel=True, win_size=3, data_range=1)  # Added data_range\n",
    "            ssim_val /= fake_ct_images.shape[0]\n",
    "            total_ssim += ssim_val\n",
    "\n",
    "            # FID\n",
    "            # Convert to [0, 255] range and to uint8\n",
    "            fake_ct_images_uint8 = ((fake_ct_images + 1) * 127.5).clamp(0, 255).byte()\n",
    "            ct_images_uint8 = ((ct_images + 1) * 127.5).clamp(0, 255).byte()\n",
    "            \n",
    "            fid_metric.update(fake_ct_images_uint8, real=True)\n",
    "            fid_metric.update(ct_images_uint8, real=False)\n",
    "            fid_value = fid_metric.compute().item()\n",
    "            total_fid += fid_value\n",
    "\n",
    "            # Perceptual Loss (VGG loss)\n",
    "            vgg_loss_value = perceptual_loss(ct_images, fake_ct_images).item()\n",
    "            total_vgg_loss += vgg_loss_value\n",
    "\n",
    "    # Average the metrics over validation set\n",
    "    avg_mse = total_mse / len(val_loader)\n",
    "    avg_ssim = total_ssim / len(val_loader)\n",
    "    avg_fid = total_fid / len(val_loader)\n",
    "    avg_vgg_loss = total_vgg_loss / len(val_loader)\n",
    "\n",
    "    return avg_mse, avg_ssim, avg_fid, avg_vgg_loss\n",
    "\n",
    "# Function to train the Pix2Pix model with early stopping\n",
    "def train_pix2pix(generator, discriminator, train_loader, val_loader, num_epochs):\n",
    "    global best_val_loss, epochs_no_improve\n",
    "    generator.train()  # Set generator to training mode\n",
    "    discriminator.train()  # Set discriminator to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (mri_images, ct_images) in enumerate(train_loader):\n",
    "            mri_images, ct_images = mri_images.to(device), ct_images.to(device)\n",
    "\n",
    "            # Labels for GAN\n",
    "            valid = torch.ones(mri_images.size(0), 1, device=device) * 0.95  # Real labels with label smoothing\n",
    "            fake = torch.zeros(mri_images.size(0), 1, device=device) * 0.95 # Fake labels\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_loss = criterion_GAN(discriminator(ct_images), valid)  # Real loss\n",
    "            fake_images = generator(mri_images)\n",
    "            fake_loss = criterion_GAN(discriminator(fake_images.detach()), fake)  # Fake loss\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, ct_images, fake_images.detach())\n",
    "            d_loss = (real_loss + fake_loss) / 2 + lambda_gp * gradient_penalty\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Calculate discriminator accuracy\n",
    "            D_real_acc = (discriminator(ct_images) > 0.5).float().mean().item()  # Accuracy on real images\n",
    "            D_fake_acc = (discriminator(fake_images.detach()) < 0.5).float().mean().item()  # Accuracy on fake images\n",
    "            D_avg_acc = (D_real_acc + D_fake_acc) / 2  # Average accuracy\n",
    "\n",
    "            # Train Generator (3 times for every 1 update of the discriminator)\n",
    "            for _ in range(2):\n",
    "                optimizer_G.zero_grad()\n",
    "                # Detach discriminator output to avoid gradient computation\n",
    "                fake_images = generator(mri_images)  # Regenerate fake images inside loop\n",
    "                disc_output = discriminator(fake_images)\n",
    "                loss_GAN = criterion_GAN(disc_output, valid)  # GAN loss\n",
    "                loss_pixel = perceptual_loss(fake_images, ct_images)  # Use perceptual loss\n",
    "                loss_G = loss_GAN + lambda_l1 * loss_pixel  # Total loss\n",
    "                loss_G.backward()  # Remove retain_graph=True\n",
    "                optimizer_G.step()\n",
    "\n",
    "            # Print detailed losses and accuracies\n",
    "            print(\".\", end=\"\")\n",
    "            if (i + 1) % 36 == 0:  # Change to every 10 batches\n",
    "                print(f\"[Epoch {epoch + 1}/{num_epochs}] [Batch {i + 1}/{len(train_loader)}] \"\n",
    "                      f\"[D Loss: {d_loss.item():.6f}] [G Loss: {loss_G.item():.6f}] \"\n",
    "                      f\"[GAN Loss: {loss_GAN.item():.6f}] [Perceptual Loss: {loss_pixel.item():.6f}] \"\n",
    "                      f\"[D Real Acc: {D_real_acc:.6f}] [D Fake Acc: {D_fake_acc:.6f}] \"\n",
    "                      f\"[D Avg Acc: {D_avg_acc:.6f}]\")\n",
    "\n",
    "        # Validation after each epoch\n",
    "        val_mse, val_ssim, val_fid, val_vgg_loss = validate(generator, val_loader)\n",
    "\n",
    "        # Log validation metrics\n",
    "        print(\"Validation -> \", end=\"\")\n",
    "        print(f\"\\n[Epoch {epoch + 1}/{num_epochs}] Validation Metrics: \"\n",
    "              f\"MSE: {val_mse:.6f}, SSIM: {val_ssim:.6f}, FID: {val_fid:.6f}, VGG Loss: {val_vgg_loss:.6f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        total_val_loss = val_fid * 0.5 + val_vgg_loss * 0.5\n",
    "        if total_val_loss < best_val_loss:\n",
    "            best_val_loss = total_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(generator.state_dict(), 'best_generator.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "            \n",
    "        # Adjust learning rates using scheduler\n",
    "        scheduler_G.step(val_fid + val_vgg_loss)\n",
    "        scheduler_D.step(d_loss)\n",
    "\n",
    "# Train the model\n",
    "train_pix2pix(generator, discriminator, train_loader, val_loader, num_epochs=60)\n",
    "\n",
    "# Save the final models\n",
    "torch.save(generator.state_dict(), 'final_generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'final_discriminator.pth')\n",
    "\n",
    "# Plotting a sample output after training\n",
    "generator.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    sample_mri = torch.tensor(val_input[0]).unsqueeze(0).to(device)  # Get the first validation MRI\n",
    "    sample_mri = sample_mri.repeat(1, 3, 1, 1)  # Repeat for 3 channels\n",
    "    generated_ct = generator(sample_mri)  # Generate CT from MRI\n",
    "\n",
    "# Convert back to numpy for plotting\n",
    "generated_ct_np = generated_ct.squeeze().cpu().numpy().transpose(1, 2, 0)  # Change dimensions for plotting\n",
    "plt.imshow((generated_ct_np + 1) / 2)  # Rescale from [-1, 1] to [0, 1]\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891da2c0-4adb-4fbe-a4ac-a2b4d7ca724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Function to visualize real and synthesized images\n",
    "def visualize_results(generator, test_loader):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        # Select 4 random images from the test dataset\n",
    "        sample_indices = random.sample(range(len(test_loader.dataset)), 4)\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for idx, i in enumerate(sample_indices):\n",
    "            mri_image, ct_image = test_loader.dataset[i]  # Get the MRI and CT images\n",
    "            mri_image = mri_image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "            generated_image = generator(mri_image).squeeze(0)  # Generate synthetic CT and remove batch dimension\n",
    "\n",
    "            # Convert images to [0, 1] range for visualization\n",
    "            mri_image = (mri_image + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n",
    "            ct_image = (ct_image + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n",
    "            generated_image = (generated_image + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n",
    "\n",
    "            # Plot MRI Image\n",
    "            plt.subplot(3, 4, idx + 1)\n",
    "            plt.imshow(mri_image.cpu().squeeze().permute(1, 2, 0), cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.title('Real MRI Image')\n",
    "\n",
    "            # Plot CT Image\n",
    "            plt.subplot(3, 4, idx + 5)\n",
    "            plt.imshow(ct_image.cpu().squeeze().permute(1, 2, 0), cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.title('Real CT Image')\n",
    "\n",
    "            # Plot Generated CT Image\n",
    "            plt.subplot(3, 4, idx + 9)\n",
    "            plt.imshow(generated_image.cpu().squeeze().permute(1, 2, 0), cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.title('Synthesized CT Image')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Call the visualization function\n",
    "visualize_results(generator, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb9e931-3bf5-4d1d-8529-bc1a2cfcff07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
