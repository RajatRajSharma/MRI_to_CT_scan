{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6a5f81-b905-4381-bdc2-61bed823e021",
   "metadata": {},
   "source": [
    "# Model MK5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "109c03cc-aaa3-4f40-b0ea-44b262eeb1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 570 pairs, Size: (570, 256, 256) (input), (570, 256, 256) (output)\n",
      "Validation set: 90 pairs, Size: (90, 256, 256) (input), (90, 256, 256) (output)\n",
      "Testing set: 150 pairs, Size: (150, 256, 256) (input), (150, 256, 256) (output)\n",
      "Training set:\n",
      " - First MRI image size: (256, 256)\n",
      " - First CT image size: (256, 256)\n",
      " - All MRI images same size: True\n",
      " - All CT images same size: True\n",
      "\n",
      "Validation set:\n",
      " - First MRI image size: (256, 256)\n",
      " - First CT image size: (256, 256)\n",
      " - All MRI images same size: True\n",
      " - All CT images same size: True\n",
      "\n",
      "Testing set:\n",
      " - First MRI image size: (256, 256)\n",
      " - First CT image size: (256, 256)\n",
      " - All MRI images same size: True\n",
      " - All CT images same size: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance  # To calculate FID\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "# Load the .npy files\n",
    "train_input = np.load('data(processed)/train_input.npy').astype(np.float32)  # Convert to float32\n",
    "train_output = np.load('data(processed)/train_output.npy').astype(np.float32)  # Convert to float32\n",
    "val_input = np.load('data(processed)/val_input.npy').astype(np.float32)\n",
    "val_output = np.load('data(processed)/val_output.npy').astype(np.float32)\n",
    "test_input = np.load('data(processed)/test_input.npy').astype(np.float32)\n",
    "test_output = np.load('data(processed)/test_output.npy').astype(np.float32)\n",
    "\n",
    "# Custom Dataset Class\n",
    "class MRICTDataset(Dataset):\n",
    "    def __init__(self, input_data, output_data, transform=None):\n",
    "        self.input_data = input_data\n",
    "        self.output_data = output_data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load MRI and CT images\n",
    "        mri_image = self.input_data[idx]\n",
    "        ct_image = self.output_data[idx]\n",
    "\n",
    "        # Convert to 3-channel images\n",
    "        mri_image = np.repeat(mri_image[np.newaxis, :, :], 3, axis=0)  # Repeat for 3 channels\n",
    "        ct_image = np.repeat(ct_image[np.newaxis, :, :], 3, axis=0)  # Repeat for 3 channels\n",
    "\n",
    "        if self.transform:\n",
    "            mri_image = torch.tensor(mri_image)  # Convert to tensor\n",
    "            ct_image = torch.tensor(ct_image)  # Convert to tensor\n",
    "\n",
    "            mri_image = self.transform(mri_image)  # Transform MRI\n",
    "            ct_image = self.transform(ct_image)  # Transform CT\n",
    "\n",
    "        return mri_image, ct_image\n",
    "\n",
    "# Data transformation with augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.float()),  # Ensure tensor is float type\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize for 3 channels\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip for data augmentation\n",
    "    transforms.RandomVerticalFlip(),  # Random vertical flip for data augmentation\n",
    "])\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = MRICTDataset(train_input, train_output, transform=transform)\n",
    "val_dataset = MRICTDataset(val_input, val_output, transform=transform)\n",
    "test_dataset = MRICTDataset(test_input, test_output, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Changed batch size to 16\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)  # Changed batch size to 16\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # Changed batch size to 16\n",
    "\n",
    "# Print counts and sizes\n",
    "print(f'Training set: {len(train_dataset)} pairs, Size: {train_input.shape} (input), {train_output.shape} (output)')\n",
    "print(f'Validation set: {len(val_dataset)} pairs, Size: {val_input.shape} (input), {val_output.shape} (output)')\n",
    "print(f'Testing set: {len(test_dataset)} pairs, Size: {test_input.shape} (input), {test_output.shape} (output)')\n",
    "\n",
    "# Check image sizes and uniformity\n",
    "def check_image_sizes(dataset, name):\n",
    "    # Get the size of the first image\n",
    "    first_input_size = dataset.input_data[0].shape\n",
    "    first_output_size = dataset.output_data[0].shape\n",
    "    \n",
    "    # Check if all images have the same size\n",
    "    all_inputs_same_size = all(img.shape == first_input_size for img in dataset.input_data)\n",
    "    all_outputs_same_size = all(img.shape == first_output_size for img in dataset.output_data)\n",
    "\n",
    "    print(f\"{name} set:\")\n",
    "    print(f\" - First MRI image size: {first_input_size}\")\n",
    "    print(f\" - First CT image size: {first_output_size}\")\n",
    "    print(f\" - All MRI images same size: {all_inputs_same_size}\")\n",
    "    print(f\" - All CT images same size: {all_outputs_same_size}\\n\")\n",
    "\n",
    "# Check sizes for training, validation, and testing datasets\n",
    "check_image_sizes(train_dataset, \"Training\")\n",
    "check_image_sizes(val_dataset, \"Validation\")\n",
    "check_image_sizes(test_dataset, \"Testing\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Define device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f5a5d7-881b-4e09-9bd2-45f5666e5c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................[Epoch 1/60] [Batch 36/36] [D Loss: 13.873836] [G Loss: 7.427912] [GAN Loss: 1.167531] [Perceptual Loss: 0.139120] [D Real Acc: 0.000000] [D Fake Acc: 1.000000] [D Avg Acc: 0.500000]\n",
      "Validation -> \n",
      "[Epoch 1/60] Validation Metrics: MSE: 0.844203, SSIM: -0.001050, FID: 315.424678, VGG Loss: 0.199854\n",
      "....................................[Epoch 2/60] [Batch 36/36] [D Loss: 14.021592] [G Loss: 5.944240] [GAN Loss: 0.568590] [Perceptual Loss: 0.119459] [D Real Acc: 0.800000] [D Fake Acc: 0.500000] [D Avg Acc: 0.650000]\n",
      "Validation -> \n",
      "[Epoch 2/60] Validation Metrics: MSE: 0.741791, SSIM: 0.000295, FID: 303.364248, VGG Loss: 0.177786\n",
      "....................................[Epoch 3/60] [Batch 36/36] [D Loss: 12.243285] [G Loss: 6.122761] [GAN Loss: 0.599707] [Perceptual Loss: 0.122735] [D Real Acc: 0.000000] [D Fake Acc: 0.900000] [D Avg Acc: 0.450000]\n",
      "Validation -> \n",
      "[Epoch 3/60] Validation Metrics: MSE: 0.603691, SSIM: 0.001460, FID: 303.636826, VGG Loss: 0.164676\n",
      "....................................[Epoch 4/60] [Batch 36/36] [D Loss: 11.855891] [G Loss: 5.186321] [GAN Loss: 0.272591] [Perceptual Loss: 0.109194] [D Real Acc: 0.500000] [D Fake Acc: 0.000000] [D Avg Acc: 0.250000]\n",
      "Validation -> \n",
      "[Epoch 4/60] Validation Metrics: MSE: 0.507120, SSIM: 0.001286, FID: 324.171234, VGG Loss: 0.158161\n",
      "....................................[Epoch 5/60] [Batch 36/36] [D Loss: 12.465254] [G Loss: 4.652567] [GAN Loss: 0.275347] [Perceptual Loss: 0.097272] [D Real Acc: 0.100000] [D Fake Acc: 0.000000] [D Avg Acc: 0.050000]\n",
      "Validation -> \n",
      "[Epoch 5/60] Validation Metrics: MSE: 0.530898, SSIM: 0.001002, FID: 271.089976, VGG Loss: 0.154659\n",
      "....................................[Epoch 6/60] [Batch 36/36] [D Loss: 10.490508] [G Loss: 4.474531] [GAN Loss: 0.263936] [Perceptual Loss: 0.093569] [D Real Acc: 0.600000] [D Fake Acc: 0.000000] [D Avg Acc: 0.300000]\n",
      "Validation -> \n",
      "[Epoch 6/60] Validation Metrics: MSE: 0.566124, SSIM: 0.000907, FID: 288.499463, VGG Loss: 0.151332\n",
      "....................................[Epoch 7/60] [Batch 36/36] [D Loss: 11.002556] [G Loss: 3.948117] [GAN Loss: 0.267461] [Perceptual Loss: 0.081792] [D Real Acc: 0.600000] [D Fake Acc: 0.000000] [D Avg Acc: 0.300000]\n",
      "Validation -> \n",
      "[Epoch 7/60] Validation Metrics: MSE: 0.582375, SSIM: 0.000592, FID: 275.541397, VGG Loss: 0.153056\n",
      "....................................[Epoch 8/60] [Batch 36/36] [D Loss: 9.377838] [G Loss: 4.476778] [GAN Loss: 0.259338] [Perceptual Loss: 0.093721] [D Real Acc: 0.700000] [D Fake Acc: 0.000000] [D Avg Acc: 0.350000]\n",
      "Validation -> \n",
      "[Epoch 8/60] Validation Metrics: MSE: 0.574730, SSIM: 0.000838, FID: 283.351301, VGG Loss: 0.148492\n",
      "....................................[Epoch 9/60] [Batch 36/36] [D Loss: 8.792316] [G Loss: 3.221910] [GAN Loss: 0.238388] [Perceptual Loss: 0.066300] [D Real Acc: 0.600000] [D Fake Acc: 0.000000] [D Avg Acc: 0.300000]\n",
      "Validation -> \n",
      "[Epoch 9/60] Validation Metrics: MSE: 0.539184, SSIM: 0.000916, FID: 245.353628, VGG Loss: 0.147903\n",
      "....................................[Epoch 10/60] [Batch 36/36] [D Loss: 8.453392] [G Loss: 3.513291] [GAN Loss: 0.250159] [Perceptual Loss: 0.072514] [D Real Acc: 0.900000] [D Fake Acc: 0.000000] [D Avg Acc: 0.450000]\n",
      "Validation -> \n",
      "[Epoch 10/60] Validation Metrics: MSE: 0.577618, SSIM: 0.000399, FID: 256.492203, VGG Loss: 0.144814\n",
      "....................................[Epoch 11/60] [Batch 36/36] [D Loss: 8.803532] [G Loss: 3.668403] [GAN Loss: 0.356848] [Perceptual Loss: 0.073590] [D Real Acc: 0.500000] [D Fake Acc: 0.000000] [D Avg Acc: 0.250000]\n",
      "Validation -> \n",
      "[Epoch 11/60] Validation Metrics: MSE: 0.523659, SSIM: 0.001834, FID: 273.020025, VGG Loss: 0.150166\n",
      "....................................[Epoch 12/60] [Batch 36/36] [D Loss: 10.741776] [G Loss: 2.944013] [GAN Loss: 0.248513] [Perceptual Loss: 0.059900] [D Real Acc: 0.700000] [D Fake Acc: 0.000000] [D Avg Acc: 0.350000]\n",
      "Validation -> \n",
      "[Epoch 12/60] Validation Metrics: MSE: 0.508789, SSIM: 0.001664, FID: 260.243629, VGG Loss: 0.154853\n",
      "....................................[Epoch 13/60] [Batch 36/36] [D Loss: 5.622697] [G Loss: 4.326672] [GAN Loss: 0.243181] [Perceptual Loss: 0.090744] [D Real Acc: 0.700000] [D Fake Acc: 0.100000] [D Avg Acc: 0.400000]\n",
      "Validation -> \n",
      "[Epoch 13/60] Validation Metrics: MSE: 0.496683, SSIM: 0.002664, FID: 256.111506, VGG Loss: 0.150020\n",
      "....................................[Epoch 14/60] [Batch 36/36] [D Loss: 4.864553] [G Loss: 3.814815] [GAN Loss: 0.357058] [Perceptual Loss: 0.076839] [D Real Acc: 0.300000] [D Fake Acc: 0.100000] [D Avg Acc: 0.200000]\n",
      "Validation -> \n",
      "[Epoch 14/60] Validation Metrics: MSE: 0.481263, SSIM: 0.002567, FID: 269.973969, VGG Loss: 0.156542\n",
      "....................................[Epoch 15/60] [Batch 36/36] [D Loss: 7.340080] [G Loss: 3.381942] [GAN Loss: 0.296252] [Perceptual Loss: 0.068571] [D Real Acc: 0.400000] [D Fake Acc: 0.000000] [D Avg Acc: 0.200000]\n",
      "Validation -> \n",
      "[Epoch 15/60] Validation Metrics: MSE: 0.478377, SSIM: 0.003299, FID: 244.291389, VGG Loss: 0.148483\n",
      "....................................[Epoch 16/60] [Batch 36/36] [D Loss: 8.931539] [G Loss: 3.628771] [GAN Loss: 0.350335] [Perceptual Loss: 0.072854] [D Real Acc: 0.400000] [D Fake Acc: 0.100000] [D Avg Acc: 0.250000]\n",
      "Validation -> \n",
      "[Epoch 16/60] Validation Metrics: MSE: 0.460145, SSIM: 0.003563, FID: 242.486900, VGG Loss: 0.151610\n",
      "....................................[Epoch 17/60] [Batch 36/36] [D Loss: 5.493993] [G Loss: 2.946123] [GAN Loss: 0.277967] [Perceptual Loss: 0.059292] [D Real Acc: 0.900000] [D Fake Acc: 0.000000] [D Avg Acc: 0.450000]\n",
      "Validation -> \n",
      "[Epoch 17/60] Validation Metrics: MSE: 0.452600, SSIM: 0.004415, FID: 232.314069, VGG Loss: 0.143500\n",
      "....................................[Epoch 18/60] [Batch 36/36] [D Loss: 8.988487] [G Loss: 2.908128] [GAN Loss: 0.336974] [Perceptual Loss: 0.057137] [D Real Acc: 0.200000] [D Fake Acc: 0.000000] [D Avg Acc: 0.100000]\n",
      "Validation -> \n",
      "[Epoch 18/60] Validation Metrics: MSE: 0.409742, SSIM: 0.005735, FID: 227.978495, VGG Loss: 0.149128\n",
      "....................................[Epoch 19/60] [Batch 36/36] [D Loss: 6.281241] [G Loss: 3.684834] [GAN Loss: 0.536910] [Perceptual Loss: 0.069954] [D Real Acc: 0.400000] [D Fake Acc: 0.300000] [D Avg Acc: 0.350000]\n",
      "Validation -> \n",
      "[Epoch 19/60] Validation Metrics: MSE: 0.440182, SSIM: 0.006269, FID: 216.098363, VGG Loss: 0.145915\n",
      "....................................[Epoch 20/60] [Batch 36/36] [D Loss: 5.379250] [G Loss: 3.524467] [GAN Loss: 0.356622] [Perceptual Loss: 0.070397] [D Real Acc: 0.500000] [D Fake Acc: 0.500000] [D Avg Acc: 0.500000]\n",
      "Validation -> \n",
      "[Epoch 20/60] Validation Metrics: MSE: 0.438589, SSIM: 0.006995, FID: 234.229497, VGG Loss: 0.145364\n",
      "....................................[Epoch 21/60] [Batch 36/36] [D Loss: 8.093937] [G Loss: 2.839697] [GAN Loss: 0.262204] [Perceptual Loss: 0.057278] [D Real Acc: 0.500000] [D Fake Acc: 0.000000] [D Avg Acc: 0.250000]\n",
      "Validation -> \n",
      "[Epoch 21/60] Validation Metrics: MSE: 0.411276, SSIM: 0.006586, FID: 230.312864, VGG Loss: 0.144472\n",
      "....................................[Epoch 22/60] [Batch 36/36] [D Loss: 6.853872] [G Loss: 3.674169] [GAN Loss: 0.330703] [Perceptual Loss: 0.074299] [D Real Acc: 0.300000] [D Fake Acc: 0.200000] [D Avg Acc: 0.250000]\n",
      "Validation -> \n",
      "[Epoch 22/60] Validation Metrics: MSE: 0.435425, SSIM: 0.006975, FID: 224.864784, VGG Loss: 0.147633\n",
      "....................................[Epoch 23/60] [Batch 36/36] [D Loss: 4.606853] [G Loss: 3.378702] [GAN Loss: 0.296812] [Perceptual Loss: 0.068486] [D Real Acc: 0.500000] [D Fake Acc: 0.000000] [D Avg Acc: 0.250000]\n",
      "Validation -> \n",
      "[Epoch 23/60] Validation Metrics: MSE: 0.418584, SSIM: 0.007390, FID: 217.617526, VGG Loss: 0.144523\n",
      "....................................[Epoch 24/60] [Batch 36/36] [D Loss: 5.308279] [G Loss: 2.633636] [GAN Loss: 0.267164] [Perceptual Loss: 0.052588] [D Real Acc: 0.400000] [D Fake Acc: 0.100000] [D Avg Acc: 0.250000]\n",
      "Validation -> \n",
      "[Epoch 24/60] Validation Metrics: MSE: 0.427775, SSIM: 0.007331, FID: 219.245885, VGG Loss: 0.151352\n",
      "....................................[Epoch 25/60] [Batch 36/36] [D Loss: 4.268137] [G Loss: 2.720126] [GAN Loss: 0.244144] [Perceptual Loss: 0.055022] [D Real Acc: 0.600000] [D Fake Acc: 0.000000] [D Avg Acc: 0.300000]\n",
      "Validation -> \n",
      "[Epoch 25/60] Validation Metrics: MSE: 0.406678, SSIM: 0.007662, FID: 212.745717, VGG Loss: 0.149786\n",
      "....................................[Epoch 26/60] [Batch 36/36] [D Loss: 3.524721] [G Loss: 2.632335] [GAN Loss: 0.248455] [Perceptual Loss: 0.052975] [D Real Acc: 0.600000] [D Fake Acc: 0.000000] [D Avg Acc: 0.300000]\n",
      "Validation -> \n",
      "[Epoch 26/60] Validation Metrics: MSE: 0.390314, SSIM: 0.008551, FID: 217.636314, VGG Loss: 0.148164\n",
      "....................................[Epoch 27/60] [Batch 36/36] [D Loss: 4.789607] [G Loss: 3.553962] [GAN Loss: 0.283018] [Perceptual Loss: 0.072688] [D Real Acc: 0.600000] [D Fake Acc: 0.000000] [D Avg Acc: 0.300000]\n",
      "Validation -> \n",
      "[Epoch 27/60] Validation Metrics: MSE: 0.382353, SSIM: 0.008315, FID: 208.268051, VGG Loss: 0.152444\n",
      "....................................[Epoch 28/60] [Batch 36/36] [D Loss: 5.855593] [G Loss: 2.928337] [GAN Loss: 0.237977] [Perceptual Loss: 0.059786] [D Real Acc: 0.600000] [D Fake Acc: 0.000000] [D Avg Acc: 0.300000]\n",
      "Validation -> \n",
      "[Epoch 28/60] Validation Metrics: MSE: 0.360235, SSIM: 0.008884, FID: 210.171000, VGG Loss: 0.144391\n",
      "....................................[Epoch 29/60] [Batch 36/36] [D Loss: 3.935098] [G Loss: 3.188725] [GAN Loss: 0.286976] [Perceptual Loss: 0.064483] [D Real Acc: 0.200000] [D Fake Acc: 0.000000] [D Avg Acc: 0.100000]\n",
      "Validation -> \n",
      "[Epoch 29/60] Validation Metrics: MSE: 0.346011, SSIM: 0.010349, FID: 201.902982, VGG Loss: 0.148845\n",
      "....................................[Epoch 30/60] [Batch 36/36] [D Loss: 11.171759] [G Loss: 57.281811] [GAN Loss: 45.052509] [Perceptual Loss: 0.271762] [D Real Acc: 0.600000] [D Fake Acc: 1.000000] [D Avg Acc: 0.800000]\n",
      "Validation -> \n",
      "[Epoch 30/60] Validation Metrics: MSE: 1.507333, SSIM: -0.001061, FID: 367.620178, VGG Loss: 0.290860\n",
      "....................................[Epoch 31/60] [Batch 36/36] [D Loss: 14.853202] [G Loss: 42.590286] [GAN Loss: 29.084906] [Perceptual Loss: 0.300120] [D Real Acc: 0.700000] [D Fake Acc: 1.000000] [D Avg Acc: 0.850000]\n",
      "Validation -> \n",
      "[Epoch 31/60] Validation Metrics: MSE: 1.339546, SSIM: 0.000356, FID: 392.811996, VGG Loss: 0.335508\n",
      "....................................[Epoch 32/60] [Batch 36/36] [D Loss: 15.284905] [G Loss: 43.098816] [GAN Loss: 28.577337] [Perceptual Loss: 0.322700] [D Real Acc: 0.800000] [D Fake Acc: 1.000000] [D Avg Acc: 0.900000]\n",
      "Validation -> \n",
      "[Epoch 32/60] Validation Metrics: MSE: 1.364747, SSIM: 0.000108, FID: 422.042775, VGG Loss: 0.332851\n",
      "....................................[Epoch 33/60] [Batch 36/36] [D Loss: 14.543818] [G Loss: 39.623825] [GAN Loss: 24.787853] [Perceptual Loss: 0.329688] [D Real Acc: 0.900000] [D Fake Acc: 1.000000] [D Avg Acc: 0.950000]\n",
      "Validation -> \n",
      "[Epoch 33/60] Validation Metrics: MSE: 1.376253, SSIM: 0.000299, FID: 418.402232, VGG Loss: 0.361540\n",
      "....................................[Epoch 34/60] [Batch 36/36] [D Loss: 11.818757] [G Loss: 43.002174] [GAN Loss: 29.703794] [Perceptual Loss: 0.295520] [D Real Acc: 0.900000] [D Fake Acc: 1.000000] [D Avg Acc: 0.950000]\n",
      "Validation -> \n",
      "[Epoch 34/60] Validation Metrics: MSE: 1.376620, SSIM: 0.000240, FID: 427.796672, VGG Loss: 0.328314\n",
      "......................"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 276\u001b[0m\n\u001b[0;32m    273\u001b[0m         scheduler_D\u001b[38;5;241m.\u001b[39mstep(d_loss)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m \u001b[43mtrain_pix2pix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# Save the final models\u001b[39;00m\n\u001b[0;32m    279\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(generator\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_generator.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 239\u001b[0m, in \u001b[0;36mtrain_pix2pix\u001b[1;34m(generator, discriminator, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[0;32m    237\u001b[0m     loss_pixel \u001b[38;5;241m=\u001b[39m perceptual_loss(fake_images, ct_images)  \u001b[38;5;66;03m# Use perceptual loss\u001b[39;00m\n\u001b[0;32m    238\u001b[0m     loss_G \u001b[38;5;241m=\u001b[39m loss_GAN \u001b[38;5;241m+\u001b[39m lambda_l1 \u001b[38;5;241m*\u001b[39m loss_pixel  \u001b[38;5;66;03m# Total loss\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     \u001b[43mloss_G\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Remove retain_graph=True\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     optimizer_G\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Print detailed losses and accuracies\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thite\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thite\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thite\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define UNet Generator\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = self.encoder_block(3, 64)\n",
    "        self.enc2 = self.encoder_block(64, 128)\n",
    "        self.enc3 = self.encoder_block(128, 256)\n",
    "        self.enc4 = self.encoder_block(256, 512)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.encoder_block(512, 512)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec4 = self.decoder_block(512, 256)\n",
    "        self.dec3 = self.decoder_block(512 + 256, 128)\n",
    "        self.dec2 = self.decoder_block(256 + 128, 64)\n",
    "        self.dec1 = nn.ConvTranspose2d(192, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.final_upsample = nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1)\n",
    "        self.tanh = nn.Tanh()  # Using Tanh for final output\n",
    "\n",
    "    def encoder_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.15),\n",
    "            nn.Dropout(p=0.1)  # Reduced dropout to 0.2 in the generator\n",
    "        )\n",
    "\n",
    "    def decoder_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.15),\n",
    "            nn.Dropout(p=0.1)  # Reduced dropout to 0.2 in the generator\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        bottleneck = self.bottleneck(enc4)\n",
    "\n",
    "        dec4 = self.dec4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec3 = self.dec3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec2 = self.dec2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec1 = self.dec1(dec2)\n",
    "        out = self.final_upsample(dec1)\n",
    "        return self.tanh(out)  # Use tanh for output\n",
    "\n",
    "# Define Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.34),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0)  # Output shape: [batch_size, 1, 1, 1]\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling to get single scalar value\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.sigmoid = nn.Sigmoid()  # Add Sigmoid layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        out = self.global_avg_pool(out)  # Apply global average pooling\n",
    "        out = out.view(out.size(0), -1)  # Flatten to [batch_size, 1]\n",
    "        out = self.sigmoid(out)  # Apply Sigmoid\n",
    "        return out\n",
    "\n",
    "# Loss functions\n",
    "criterion_GAN = nn.BCELoss()  # GAN loss\n",
    "criterion_pixelwise = nn.L1Loss()  # L1 loss\n",
    "vgg = models.vgg19(weights='DEFAULT').features.eval().to(device)  # Pre-trained VGG for perceptual loss\n",
    "\n",
    "# Frechet Inception Distance (FID) for validation\n",
    "fid_metric = FrechetInceptionDistance().to(device)\n",
    "\n",
    "# Custom perceptual loss function\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    # Compute the VGG features\n",
    "    y_true_features = vgg(y_true)\n",
    "    y_pred_features = vgg(y_pred)\n",
    "    return nn.functional.mse_loss(y_pred_features, y_true_features)  # Use MSE for feature loss\n",
    "\n",
    "# Function to compute gradient penalty\n",
    "def compute_gradient_penalty(discriminator, real_images, fake_images):\n",
    "    batch_size = real_images.size(0)\n",
    "    # Generate random weight for interpolation\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1).to(real_images.device)\n",
    "    # Interpolate between real and fake images\n",
    "    interpolates = (alpha * real_images + (1 - alpha) * fake_images).requires_grad_(True)\n",
    "    \n",
    "    # Get discriminator output for interpolated images\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    \n",
    "    # Compute gradients of the output with respect to the interpolated images\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(d_interpolates),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "\n",
    "    # Compute the L2 norm of the gradients\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# Hyperparameter for balancing losses\n",
    "lambda_l1 = 45  # Decreased lambda_l1\n",
    "lambda_gp = 15  # Weight for gradient penalty\n",
    "\n",
    "# Early stopping variables\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Initialize models\n",
    "generator = UNetGenerator().to(device)\n",
    "discriminator = Discriminator(in_channels=3).to(device)\n",
    "\n",
    "# Optimizers                                     step size = lr * gradient\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.00018, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.000055, betas=(0.5, 0.999))  # Updated D learning rate\n",
    "\n",
    "# Learning rate scheduler with patience\n",
    "scheduler_G = optim.lr_scheduler.ReduceLROnPlateau(optimizer_G, mode='min', factor=0.5, patience=5)\n",
    "scheduler_D = optim.lr_scheduler.ReduceLROnPlateau(optimizer_D, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Validation function to calculate metrics\n",
    "def validate(generator, val_loader):\n",
    "    generator.eval()  # Set to evaluation mode\n",
    "    total_mse, total_ssim, total_fid, total_vgg_loss = 0, 0, 0, 0\n",
    "    fid_metric.reset()  # Reset FID metric\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mri_images, ct_images in val_loader:\n",
    "            mri_images, ct_images = mri_images.to(device), ct_images.to(device)\n",
    "\n",
    "            # Generate CT images from MRI\n",
    "            fake_ct_images = generator(mri_images)\n",
    "\n",
    "            # MSE\n",
    "            mse = mse_loss(fake_ct_images, ct_images).item()\n",
    "            total_mse += mse\n",
    "\n",
    "            # SSIM\n",
    "            ssim_val = 0\n",
    "            for i in range(fake_ct_images.shape[0]):\n",
    "                # Adjusting SSIM calculation\n",
    "                ssim_val += ssim(fake_ct_images[i].cpu().numpy(), ct_images[i].cpu().numpy(), \n",
    "                                  multichannel=True, win_size=3, data_range=1)  # Added data_range\n",
    "            ssim_val /= fake_ct_images.shape[0]\n",
    "            total_ssim += ssim_val\n",
    "\n",
    "            # FID\n",
    "            # Convert to [0, 255] range and to uint8\n",
    "            fake_ct_images_uint8 = ((fake_ct_images + 1) * 127.5).clamp(0, 255).byte()\n",
    "            ct_images_uint8 = ((ct_images + 1) * 127.5).clamp(0, 255).byte()\n",
    "            \n",
    "            fid_metric.update(fake_ct_images_uint8, real=True)\n",
    "            fid_metric.update(ct_images_uint8, real=False)\n",
    "            fid_value = fid_metric.compute().item()\n",
    "            total_fid += fid_value\n",
    "\n",
    "            # Perceptual Loss (VGG loss)\n",
    "            vgg_loss_value = perceptual_loss(ct_images, fake_ct_images).item()\n",
    "            total_vgg_loss += vgg_loss_value\n",
    "\n",
    "    # Average the metrics over validation set\n",
    "    avg_mse = total_mse / len(val_loader)\n",
    "    avg_ssim = total_ssim / len(val_loader)\n",
    "    avg_fid = total_fid / len(val_loader)\n",
    "    avg_vgg_loss = total_vgg_loss / len(val_loader)\n",
    "\n",
    "    return avg_mse, avg_ssim, avg_fid, avg_vgg_loss\n",
    "\n",
    "# Function to train the Pix2Pix model with early stopping\n",
    "def train_pix2pix(generator, discriminator, train_loader, val_loader, num_epochs):\n",
    "    global best_val_loss, epochs_no_improve\n",
    "    generator.train()  # Set generator to training mode\n",
    "    discriminator.train()  # Set discriminator to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (mri_images, ct_images) in enumerate(train_loader):\n",
    "            mri_images, ct_images = mri_images.to(device), ct_images.to(device)\n",
    "\n",
    "            # Labels for GAN\n",
    "            valid = torch.ones(mri_images.size(0), 1, device=device) * 0.95  # Real labels with label smoothing\n",
    "            fake = torch.zeros(mri_images.size(0), 1, device=device) * 0.95 # Fake labels\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            real_loss = criterion_GAN(discriminator(ct_images), valid)  # Real loss\n",
    "            fake_images = generator(mri_images)\n",
    "            fake_loss = criterion_GAN(discriminator(fake_images.detach()), fake)  # Fake loss\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, ct_images, fake_images.detach())\n",
    "            d_loss = (real_loss + fake_loss) / 2 + lambda_gp * gradient_penalty\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Calculate discriminator accuracy\n",
    "            D_real_acc = (discriminator(ct_images) > 0.5).float().mean().item()  # Accuracy on real images\n",
    "            D_fake_acc = (discriminator(fake_images.detach()) < 0.5).float().mean().item()  # Accuracy on fake images\n",
    "            D_avg_acc = (D_real_acc + D_fake_acc) / 2  # Average accuracy\n",
    "\n",
    "            # Train Generator (3 times for every 1 update of the discriminator)\n",
    "            for _ in range(2):\n",
    "                optimizer_G.zero_grad()\n",
    "                # Detach discriminator output to avoid gradient computation\n",
    "                fake_images = generator(mri_images)  # Regenerate fake images inside loop\n",
    "                disc_output = discriminator(fake_images)\n",
    "                loss_GAN = criterion_GAN(disc_output, valid)  # GAN loss\n",
    "                loss_pixel = perceptual_loss(fake_images, ct_images)  # Use perceptual loss\n",
    "                loss_G = loss_GAN + lambda_l1 * loss_pixel  # Total loss\n",
    "                loss_G.backward()  # Remove retain_graph=True\n",
    "                optimizer_G.step()\n",
    "\n",
    "            # Print detailed losses and accuracies\n",
    "            print(\".\", end=\"\")\n",
    "            if (i + 1) % 36 == 0:  # Change to every 10 batches\n",
    "                print(f\"[Epoch {epoch + 1}/{num_epochs}] [Batch {i + 1}/{len(train_loader)}] \"\n",
    "                      f\"[D Loss: {d_loss.item():.6f}] [G Loss: {loss_G.item():.6f}] \"\n",
    "                      f\"[GAN Loss: {loss_GAN.item():.6f}] [Perceptual Loss: {loss_pixel.item():.6f}] \"\n",
    "                      f\"[D Real Acc: {D_real_acc:.6f}] [D Fake Acc: {D_fake_acc:.6f}] \"\n",
    "                      f\"[D Avg Acc: {D_avg_acc:.6f}]\")\n",
    "\n",
    "        # Validation after each epoch\n",
    "        val_mse, val_ssim, val_fid, val_vgg_loss = validate(generator, val_loader)\n",
    "\n",
    "        # Log validation metrics\n",
    "        print(\"Validation -> \", end=\"\")\n",
    "        print(f\"\\n[Epoch {epoch + 1}/{num_epochs}] Validation Metrics: \"\n",
    "              f\"MSE: {val_mse:.6f}, SSIM: {val_ssim:.6f}, FID: {val_fid:.6f}, VGG Loss: {val_vgg_loss:.6f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        total_val_loss = val_fid * 0.5 + val_vgg_loss * 0.5\n",
    "        if total_val_loss < best_val_loss:\n",
    "            best_val_loss = total_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(generator.state_dict(), 'best_generator.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "            \n",
    "        # Adjust learning rates using scheduler\n",
    "        scheduler_G.step(val_fid + val_vgg_loss)\n",
    "        scheduler_D.step(d_loss)\n",
    "\n",
    "# Train the model\n",
    "train_pix2pix(generator, discriminator, train_loader, val_loader, num_epochs=60)\n",
    "\n",
    "# Save the final models\n",
    "torch.save(generator.state_dict(), 'final_generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'final_discriminator.pth')\n",
    "\n",
    "# Plotting a sample output after training\n",
    "generator.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    sample_mri = torch.tensor(val_input[0]).unsqueeze(0).to(device)  # Get the first validation MRI\n",
    "    sample_mri = sample_mri.repeat(1, 3, 1, 1)  # Repeat for 3 channels\n",
    "    generated_ct = generator(sample_mri)  # Generate CT from MRI\n",
    "\n",
    "# Convert back to numpy for plotting\n",
    "generated_ct_np = generated_ct.squeeze().cpu().numpy().transpose(1, 2, 0)  # Change dimensions for plotting\n",
    "plt.imshow((generated_ct_np + 1) / 2)  # Rescale from [-1, 1] to [0, 1]\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891da2c0-4adb-4fbe-a4ac-a2b4d7ca724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Function to visualize real and synthesized images\n",
    "def visualize_results(generator, test_loader):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        # Select 4 random images from the test dataset\n",
    "        sample_indices = random.sample(range(len(test_loader.dataset)), 4)\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for idx, i in enumerate(sample_indices):\n",
    "            mri_image, ct_image = test_loader.dataset[i]  # Get the MRI and CT images\n",
    "            mri_image = mri_image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "            generated_image = generator(mri_image).squeeze(0)  # Generate synthetic CT and remove batch dimension\n",
    "\n",
    "            # Convert images to [0, 1] range for visualization\n",
    "            mri_image = (mri_image + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n",
    "            ct_image = (ct_image + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n",
    "            generated_image = (generated_image + 1) / 2  # Rescale from [-1, 1] to [0, 1]\n",
    "\n",
    "            # Plot MRI Image\n",
    "            plt.subplot(3, 4, idx + 1)\n",
    "            plt.imshow(mri_image.cpu().squeeze().permute(1, 2, 0), cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.title('Real MRI Image')\n",
    "\n",
    "            # Plot CT Image\n",
    "            plt.subplot(3, 4, idx + 5)\n",
    "            plt.imshow(ct_image.cpu().squeeze().permute(1, 2, 0), cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.title('Real CT Image')\n",
    "\n",
    "            # Plot Generated CT Image\n",
    "            plt.subplot(3, 4, idx + 9)\n",
    "            plt.imshow(generated_image.cpu().squeeze().permute(1, 2, 0), cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.title('Synthesized CT Image')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Call the visualization function\n",
    "visualize_results(generator, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb9e931-3bf5-4d1d-8529-bc1a2cfcff07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
